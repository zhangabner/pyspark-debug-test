{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('camilo', 'colombia'), ('maria', 'colombia')]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"camilo\", \"colombia\"),\n",
    "    (\"maria\", \"colombia\")\n",
    "]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hello = Hola!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Hola!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import scala.collection.JavaConverters._\n",
    "// val spark: SparkContext = new SparkContext()\n",
    "\n",
    "// VALUES are immutable constants. You can't change them once defined.\n",
    "val hello: String = \"Hola!\"                     //> hello  : String = Hola!\n",
    "println(hello)                                  //> Hola!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = false)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [firstname#50, middlename#51, lastname#52, dob#53, gender#54, salary#55]\n",
      "+- *(1) Filter (isnotnull(salary#55) && (salary#55 >= 4000))\n",
      "   +- *(1) FileScan parquet [firstname#50,middlename#51,lastname#52,dob#53,gender#54,salary#55] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/output/people.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(salary), GreaterThanOrEqual(salary,4000)], ReadSchema: struct<firstname:string,middlename:string,lastname:string,dob:string,gender:string,salary:int>\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [firstname#100,middlename#101,lastname#102,dob#103,gender#104,salary#105] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/output/people2.parquet], PartitionCount: 1, PartitionFilters: [isnotnull(gender#104), isnotnull(salary#105), (gender#104 = M), (salary#105 >= 4000)], PushedFilters: [], ReadSchema: struct<firstname:string,middlename:string,lastname:string,dob:string>\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|  dob|salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|  Robert |          |Williams|42114|  4000|\n",
      "| Michael |      Rose|        |40288|  4000|\n",
      "|   James |          |   Smith|36636|  3000|\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@73af0f58\n",
       "data = List((\"James \",\"\",Smith,36636,M,3000), (\"Michael \",Rose,\"\",40288,M,4000), (\"Robert \",\"\",Williams,42114,M,4000), (\"Maria \",Anne,Jones,39192,F,4000), (Jen,Mary,Brown,\"\",F,-1))\n",
       "columns = List(firstname, middlename, lastname, dob, gender, salary)\n",
       "df = [firstname: string, middlename: string ... 4 more fields]\n",
       "parqDF = [firstname: string, middlename: string ... 4 more fields]\n",
       "parkSQL = [firstname: string, middlename: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "parqDF2: org.apache.spark...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[firstname: string, middlename: string ... 4 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark: SparkSession = SparkSession.builder()\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"SparkByExamples.com\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val data = Seq((\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "             (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "             (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "             (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "             (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1))\n",
    "\n",
    "val columns = Seq(\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\")\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "val df = data.toDF(columns:_*)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.write\n",
    "  .parquet(\"/tmp/output/people.parquet\")\n",
    "val parqDF = spark.read.parquet(\"/tmp/output/people.parquet\")\n",
    "\n",
    "parqDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "spark.sql(\"select * from ParquetTable where salary >= 4000\").explain()\n",
    "val parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "parkSQL.show()\n",
    "parkSQL.printSchema()\n",
    "df.write\n",
    "  .partitionBy(\"gender\",\"salary\")\n",
    "  .parquet(\"/tmp/output/people2.parquet\")\n",
    "val parqDF2 = spark.read.parquet(\"/tmp/output/people2.parquet\")\n",
    "parqDF2.createOrReplaceTempView(\"ParquetTable2\")\n",
    "val df3 = spark.sql(\"select * from ParquetTable2  where gender='M' and salary >= 4000\")\n",
    "df3.explain()\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "val parqDF3 = spark.read\n",
    "  .parquet(\"/tmp/output/people2.parquet/gender=M\")\n",
    "parqDF3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True, {'model_version': 3})\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|first_name| country|\n",
      "+----------+--------+\n",
      "|    camilo|colombia|\n",
      "|     maria|colombia|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first_name (StringType): {}', \"country (StringType): {'model_version': 3}\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"%s (%s): %s\" % (t.name, t.dataType, t.metadata) for t in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet('tmp/people_with_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/matthewpowers/Documents/code/my_apps/pysparktestingexample/tmp/people_with_metadata/part-00000-b726eeb0-5ce2-4726-a9da-f721f62007e2-c000.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pq.read_table(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "first_name: string\n",
      "country: string\n"
     ]
    }
   ],
   "source": [
    "print(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Field<country: string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file.schema.field('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'PARQUET:field_id': b'2'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file.schema.field('country').metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "b'model_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c61f033c88b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparquet_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'model_version'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: b'model_version'"
     ]
    }
   ],
   "source": [
    "parquet_file.schema.field('country').metadata[b'model_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
