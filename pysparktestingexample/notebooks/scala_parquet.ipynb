{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hello = Hola!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Hola!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import scala.collection.JavaConverters._\n",
    "// val spark: SparkContext = new SparkContext()\n",
    "\n",
    "// VALUES are immutable constants. You can't change them once defined.\n",
    "val hello: String = \"Hola!\"                     //> hello  : String = Hola!\n",
    "println(hello)                                  //> Hola!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "path file:/tmp/output/people.parquet already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: path file:/tmp/output/people.parquet already exists.;",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)",
      "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)",
      "  ... 50 elided"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark: SparkSession = SparkSession.builder()\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"SparkByExamples.com\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val data = Seq((\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "             (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "             (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "             (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "             (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1))\n",
    "\n",
    "val columns = Seq(\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\")\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "val df = data.toDF(columns:_*)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.write\n",
    "  .parquet(\"/tmp/output/people.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [firstname#214, middlename#215, lastname#216, dob#217, gender#218, salary#219]\n",
      "+- *(1) Filter (isnotnull(salary#219) && (salary#219 >= 4000))\n",
      "   +- *(1) FileScan parquet [firstname#214,middlename#215,lastname#216,dob#217,gender#218,salary#219] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/output/people.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(salary), GreaterThanOrEqual(salary,4000)], ReadSchema: struct<firstname:string,middlename:string,lastname:string,dob:string,gender:string,salary:int>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parqDF = [firstname: string, middlename: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[firstname: string, middlename: string ... 4 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parqDF = spark.read.parquet(\"/tmp/output/people.parquet\")\n",
    "\n",
    "parqDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "spark.sql(\"select * from ParquetTable where salary >= 4000\").explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "path file:/tmp/output/people2.parquet already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: path file:/tmp/output/people2.parquet already exists.;",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)",
      "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)",
      "  ... 50 elided"
     ]
    }
   ],
   "source": [
    "val parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "parkSQL.show()\n",
    "parkSQL.printSchema()\n",
    "df.write\n",
    "  .partitionBy(\"gender\",\"salary\")\n",
    "  .parquet(\"/tmp/output/people2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [firstname#264,middlename#265,lastname#266,dob#267,gender#268,salary#269] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/output/people2.parquet], PartitionCount: 1, PartitionFilters: [isnotnull(gender#268), isnotnull(salary#269), (gender#268 = M), (salary#269 >= 4000)], PushedFilters: [], ReadSchema: struct<firstname:string,middlename:string,lastname:string,dob:string>\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parqDF2 = [firstname: string, middlename: string ... 4 more fields]\n",
       "df3 = [firstname: string, middlename: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[firstname: string, middlename: string ... 4 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parqDF2 = spark.read.parquet(\"/tmp/output/people2.parquet\")\n",
    "parqDF2.createOrReplaceTempView(\"ParquetTable2\")\n",
    "val df3 = spark.sql(\"select * from ParquetTable2  where gender='M' and salary >= 4000\")\n",
    "df3.explain()\n",
    "df3.printSchema()\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|  dob|salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|  Robert |          |Williams|42114|  4000|\n",
      "| Michael |      Rose|        |40288|  4000|\n",
      "|   James |          |   Smith|36636|  3000|\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parqDF3 = [firstname: string, middlename: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[firstname: string, middlename: string ... 3 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parqDF3 = spark.read\n",
    "  .parquet(\"/tmp/output/people2.parquet/gender=M\")\n",
    "parqDF3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
